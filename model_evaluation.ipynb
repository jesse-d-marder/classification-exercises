{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de34af5-a3f2-4d3c-ac21-f79cf6b77aae",
   "metadata": {},
   "source": [
    "### Exercises from https://ds.codeup.com/classification/evaluation/#exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d340abd7-4b5d-4c5c-bf0e-cef6fd0b0370",
   "metadata": {},
   "source": [
    "## EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "319088e8-2bbe-47d3-8856-6d7d08d93fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec575792-df31-4434-a944-4691cde9529e",
   "metadata": {},
   "source": [
    "### 2. Confusion matrix:\n",
    "|               | pred dog   | pred cat   |\n",
    "|:------------  |-----------:|-----------:|\n",
    "| actual dog    |         46 |         7  |\n",
    "| actual cat    |         13 |         34 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc5b73-11a5-4b0e-b253-185240c156e7",
   "metadata": {},
   "source": [
    "False Positive: Predict cat but actually a dog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a1c271-0549-4c17-85f3-1932d2e681f2",
   "metadata": {},
   "source": [
    "False Negative: Predict dog but actually a cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ced4a1-bb17-43b9-99dd-f892f0dcea04",
   "metadata": {},
   "source": [
    "True Positive: Predicted cat and is a cat\n",
    "\n",
    "True Negative: Predicted dog and is a dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "000f627e-c2d9-4a21-b426-9427852ae12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = 7\n",
    "FN = 13\n",
    "TP = 34\n",
    "TN = 46"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7998db4f-1f35-4f8a-a755-9a6ec6c6e060",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "- **accuracy**: (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "- **precision**: TP / (TP + FP)\n",
    "\n",
    "- **recall**: TP / (TP + FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c546b81c-2770-4fb9-ae85-1dac2a4ce7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8, Precision: 0.8292682926829268, Recall: 0.723404255319149\n"
     ]
    }
   ],
   "source": [
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5239b794-0dc7-4f13-89e2-d9e5da569099",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Rubber ducks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a32830a8-a21f-404e-9a3e-9139a486ecfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        actual     model1     model2     model3\n",
       "0    No Defect  No Defect     Defect  No Defect\n",
       "1    No Defect  No Defect     Defect     Defect\n",
       "2    No Defect  No Defect     Defect  No Defect\n",
       "3    No Defect     Defect     Defect     Defect\n",
       "4    No Defect  No Defect     Defect  No Defect\n",
       "..         ...        ...        ...        ...\n",
       "195  No Defect  No Defect     Defect     Defect\n",
       "196     Defect     Defect  No Defect  No Defect\n",
       "197  No Defect  No Defect  No Defect  No Defect\n",
       "198  No Defect  No Defect     Defect     Defect\n",
       "199  No Defect  No Defect  No Defect     Defect\n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "df = pd.read_csv('c3.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e349bf-0645-4e8d-b3ca-6184681a4956",
   "metadata": {},
   "source": [
    "### Want to identify as many ducks with defect as possible. Which metric to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d9fb88-b603-4e9e-849e-67c7f7ba7bbc",
   "metadata": {},
   "source": [
    "- Positive: defect\n",
    "- Negative: no defect\n",
    "\n",
    "- False positive: predict defect but no defect\n",
    "- False negative: predict no defect but defect\n",
    "\n",
    "Less worried about false positives, more concerned abour false negatives!!! Don't want to let a defective duck slip by...\n",
    "\n",
    "###  -> USE RECALL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660bb2b5-2d6b-400e-800a-ed349838faa3",
   "metadata": {},
   "source": [
    "### Which model to go with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37e1be98-f316-4772-8876-300ab62ffac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1\n",
      "   model accuracy: 95.00%\n",
      "\n",
      "   model recall: 50.00%\n",
      "\n",
      "model precision: 80.00%\n",
      "--------\n",
      "model2\n",
      "   model accuracy: 56.00%\n",
      "\n",
      "   model recall: 56.25%\n",
      "\n",
      "model precision: 10.00%\n",
      "--------\n",
      "model3\n",
      "   model accuracy: 55.50%\n",
      "\n",
      "   model recall: 81.25%\n",
      "\n",
      "model precision: 13.13%\n",
      "--------\n",
      "The model with the highest recall is model3\n",
      "The model with the highest precision is model1\n"
     ]
    }
   ],
   "source": [
    "positive = 'Defect'\n",
    "models = ['model1','model2','model3']\n",
    "recalls={}\n",
    "precisions={}\n",
    "for model in models:\n",
    "    \n",
    "    # accuracy -- overall hit rate\n",
    "    model_accuracy = (df[model]== df.actual).mean()\n",
    "    # baseline_accuracy = (df.baseline == df.actual).mean()\n",
    "\n",
    "    # precision -- how good are our positive predictions?\n",
    "    # precision -- model performance | predicted positive\n",
    "    subset = df[df[model] == positive]\n",
    "    model_precision = (subset[model] == subset.actual).mean()\n",
    "    precisions[model] = model_precision\n",
    "    # subset = df[df.baseline == positive]\n",
    "    # baseline_precision = (subset.baseline == subset.actual).mean()\n",
    "\n",
    "    # recall -- how good are we at detecting actual positives?\n",
    "    # recall -- model performance | actual positive\n",
    "    subset = df[df.actual == positive]\n",
    "    model_recall = (subset[model] == subset.actual).mean()\n",
    "    recalls[model]=model_recall\n",
    "    # baseline_recall = (subset.baseline == subset.actual).mean()\n",
    "\n",
    "    print(f'{model}')\n",
    "    print(f'   model accuracy: {model_accuracy:.2%}')\n",
    "    # print(f'baseline accuracy: {baseline_accuracy:.2%}')\n",
    "    print()\n",
    "    print(f'   model recall: {model_recall:.2%}')\n",
    "    # print(f'baseline recall: {baseline_recall:.2%}')\n",
    "    print()\n",
    "    print(f'model precision: {model_precision:.2%}')\n",
    "    # print(f'baseline precision: {baseline_precision:.2%}')\n",
    "    print('--------')\n",
    "\n",
    "print(f'The model with the highest recall is {max(recalls, key = recalls.get)}')\n",
    "print(f'The model with the highest precision is {max(precisions, key = precisions.get)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0a147f-7e7a-419b-ae04-881d907a2437",
   "metadata": {},
   "source": [
    "## -> USE MODEL 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4f623-54d8-4d95-baaf-70ff118eec44",
   "metadata": {},
   "source": [
    "### Going to give a vacation for those with defective duck\n",
    "### Really don't want to accidentally give someone a vacation package if it's not defective\n",
    "#### Reminder that false positive is predict positive but actually negative, false negative is predict no defect but actually is defective\n",
    "#### False positive is more expensive -> optimize for precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eaa644a4-4aa9-4d8b-9573-e62d4a7092ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model with the highest precision is model1\n"
     ]
    }
   ],
   "source": [
    "print(f'The model with the highest precision is {max(precisions, key = precisions.get)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315a0192-b0ad-4213-b112-6268e1d383d8",
   "metadata": {},
   "source": [
    "## 4. Given the Gives you paws dataset use pandas to create a baseline model (i.e. a model that just predicts the most common class) and answer the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a49d243c-7182-4be5-ab83-8d512518fd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('gives_you_paws.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfabd45b-eb53-4356-a6ef-5049289fa72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4\n",
       "0    cat    cat    dog    cat    dog\n",
       "1    dog    dog    cat    cat    dog\n",
       "2    dog    cat    cat    cat    dog\n",
       "3    dog    dog    dog    cat    dog\n",
       "4    cat    cat    cat    dog    dog"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02bc2b55-1cb6-4fcf-8438-dd9d27c4ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = df.actual.value_counts().idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d9cc43-27e9-43ab-87ff-2b655108f9d2",
   "metadata": {},
   "source": [
    "- positive = cat\n",
    "- negative = dog\n",
    "\n",
    "- false positive = predicts cat but dog\n",
    "- false negative = predicts dog but cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a5ad6ca6-b3d2-4e60-8582-523396020f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['baseline'] = baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "61dcc344-2520-4a23-a378-ca9d92db2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2bc6620a-b999-4329-8346-c0a722ce19ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.remove('actual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ebf7f67d-3760-4cfc-b93e-0c65f027670a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model1', 'model2', 'model3', 'model4', 'baseline']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "98140f34-41a1-471f-ad38-23a34dff3237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1\n",
      "   model accuracy: 80.74%\n",
      "   model recall: 81.50%\n",
      "   model precision: 68.98%\n",
      "model2\n",
      "   model accuracy: 63.04%\n",
      "   model recall: 89.06%\n",
      "   model precision: 48.41%\n",
      "model3\n",
      "   model accuracy: 50.96%\n",
      "   model recall: 51.15%\n",
      "   model precision: 35.83%\n",
      "model4\n",
      "   model accuracy: 74.26%\n",
      "   model recall: 34.54%\n",
      "   model precision: 80.72%\n",
      "baseline\n",
      "   model accuracy: 65.08%\n",
      "   model recall: 0.00%\n",
      "   model precision: nan%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.807400</td>\n",
       "      <td>0.630400</td>\n",
       "      <td>0.509600</td>\n",
       "      <td>0.742600</td>\n",
       "      <td>0.6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.689772</td>\n",
       "      <td>0.484122</td>\n",
       "      <td>0.358347</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.815006</td>\n",
       "      <td>0.890607</td>\n",
       "      <td>0.511455</td>\n",
       "      <td>0.345361</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model1    model2    model3    model4  baseline\n",
       "accuracy   0.807400  0.630400  0.509600  0.742600    0.6508\n",
       "precision  0.689772  0.484122  0.358347  0.807229       NaN\n",
       "recall     0.815006  0.890607  0.511455  0.345361    0.0000"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive = 'cat'\n",
    "\n",
    "models = df.columns.to_list()\n",
    "models.remove('actual')\n",
    "accuracies= {}\n",
    "precisions = {}\n",
    "recalls = {}\n",
    "\n",
    "for model in models:\n",
    "    # accuracy -- overall hit rate\n",
    "    model_accuracy = (df[model] == df.actual).mean()\n",
    "\n",
    "    # precision -- how good are our positive predictions?\n",
    "    # precision -- model performance | predicted positive\n",
    "    subset = df[df[model] == positive]\n",
    "    model_precision = (subset[model] == subset.actual).mean()\n",
    "\n",
    "    # recall -- how good are we at detecting actual positives?\n",
    "    # recall -- model performance | actual positive\n",
    "    subset = df[df.actual == positive]\n",
    "    model_recall = (subset[model] == subset.actual).mean()\n",
    "\n",
    "    accuracies[model] = model_accuracy\n",
    "    precisions[model] = model_precision\n",
    "    recalls[model] = model_recall\n",
    "        \n",
    "    print(model)\n",
    "    print(f'   model accuracy: {model_accuracy:.2%}')\n",
    "    print(f'   model recall: {model_recall:.2%}')\n",
    "    print(f'   model precision: {model_precision:.2%}')\n",
    "\n",
    "results = pd.DataFrame(data = [accuracies, precisions, recalls], index = ['accuracy','precision','recall'])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c872ac94-7365-4708-822b-47525111f24c",
   "metadata": {},
   "source": [
    "In terms of accuracy, how do the various models compare to the baseline model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6298dd8c-0308-4cd0-95ea-fbcc61137427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models better than baseline based on accuracy: ['model1', 'model4']\n",
      "Models worse than baseline based on accuracy: ['model2', 'model3']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Models better than baseline based on accuracy: {results.columns[results.loc['accuracy'].baseline<results.loc['accuracy']].to_list()}\")\n",
    "print(f\"Models worse than baseline based on accuracy: {results.columns[results.loc['accuracy'].baseline>results.loc['accuracy']].to_list()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b21c9c-08a0-4a0b-b62a-e8d74143c791",
   "metadata": {},
   "source": [
    "Are any of the models better than the baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c65685a5-0f35-4713-9f1b-e0046553a569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models better than baseline based on accuracy: ['model1', 'model4']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Models better than baseline based on accuracy: {results.columns[results.loc['accuracy'].baseline<results.loc['accuracy']].to_list()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3316ebd2-fe90-4c73-9de3-1c381bcf2cea",
   "metadata": {},
   "source": [
    "- positive = cat\n",
    "- negative = dog\n",
    "\n",
    "- false positive = predicts cat but dog\n",
    "- false negative = predicts dog but cat\n",
    "\n",
    "- Phase 1: determine dog or cat\n",
    "- Phase 2: Another review, presented to users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a262c58c-3e77-4175-bf1e-fd98309f29b7",
   "metadata": {},
   "source": [
    "Suppose you are working on a team that solely deals with dog pictures. Which of these models would you recomend for Phase I? For Phase II?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4183ec-72c7-477b-844f-85d72fab3073",
   "metadata": {},
   "source": [
    "#### I want Phase I model to let more actual cats through but predicted dogs so we can correct - happier with a false negative. False positive more expensive as then I don't have chance to correct. Optimize for precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fc85ae6d-be5d-4d19-a0d1-13d7df42c250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model based on precision: model4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best model based on precision: {max(precisions, key = precisions.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f225565-e7a4-4489-be1d-4369c4b22003",
   "metadata": {},
   "source": [
    "#### I want Phase II model to avoid sending any cats through - happier with a false positive. False negative more expensive as then the customer sees a cat!! Optimize for recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "516853f9-8fba-46f1-88ce-0c77e045985c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model based on recall: model2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best model based on recall: {max(recalls, key = recalls.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d656646-e361-44e8-8399-7fe8cafb06e6",
   "metadata": {},
   "source": [
    "Suppose you are working on a team that solely deals with cat pictures. Which of these models would you recomend for Phase I? For Phase II?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add2fe1d-3a7b-4ddc-bd21-2561f83bd7f0",
   "metadata": {},
   "source": [
    "#### I want Phase I model to let more actual dogs through but predicted cats so we can correct - happier with a false positive. False negative more expensive as then I don't have chance to correct. Optimize for recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fbdfcbca-bff6-4d09-a0cd-49e076fdc038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model based on recall: model2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best model based on recall: {max(recalls, key = recalls.get)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a702c78d-7aa3-4406-ada3-2b643f4a6487",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### I want Phase II model to avoid sending any dogs through - happier with a false negative. False positive more expensive as then the customer sees a dog!! Optimize for precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1fcec92d-608b-413b-bda3-dfbd5c83790b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model based on precision: model4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best model based on precision: {max(precisions, key = precisions.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d641cd1c-556c-47c4-8d1c-8d06303fae36",
   "metadata": {},
   "source": [
    "### Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6dd25c7f-cf12-4367-a425-77c1781029d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1\n",
      "   model accuracy: 80.74%\n",
      "   model recall: 81.50%\n",
      "   model precision: 68.98%\n",
      "model2\n",
      "   model accuracy: 63.04%\n",
      "   model recall: 89.06%\n",
      "   model precision: 48.41%\n",
      "model3\n",
      "   model accuracy: 50.96%\n",
      "   model recall: 51.15%\n",
      "   model precision: 35.83%\n",
      "model4\n",
      "   model accuracy: 74.26%\n",
      "   model recall: 34.54%\n",
      "   model precision: 80.72%\n",
      "baseline\n",
      "   model accuracy: 65.08%\n",
      "   model recall: 0.00%\n",
      "   model precision: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.807400</td>\n",
       "      <td>0.630400</td>\n",
       "      <td>0.509600</td>\n",
       "      <td>0.742600</td>\n",
       "      <td>0.6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.689772</td>\n",
       "      <td>0.484122</td>\n",
       "      <td>0.358347</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.815006</td>\n",
       "      <td>0.890607</td>\n",
       "      <td>0.511455</td>\n",
       "      <td>0.345361</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model1    model2    model3    model4  baseline\n",
       "accuracy   0.807400  0.630400  0.509600  0.742600    0.6508\n",
       "precision  0.689772  0.484122  0.358347  0.807229    0.0000\n",
       "recall     0.815006  0.890607  0.511455  0.345361    0.0000"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive = 'cat'\n",
    "\n",
    "models = df.columns.to_list()\n",
    "models.remove('actual')\n",
    "accuracies= {}\n",
    "precisions = {}\n",
    "recalls = {}\n",
    "\n",
    "for model in models:\n",
    "    # accuracy -- overall hit rate\n",
    "    model_accuracy = accuracy_score(df.actual, df[model])\n",
    "\n",
    "    # precision -- how good are our positive predictions?\n",
    "    # precision -- model performance | predicted positive\n",
    "    model_precision = precision_score(df.actual, df[model], pos_label=positive)\n",
    "\n",
    "    # recall -- how good are we at detecting actual positives?\n",
    "    # recall -- model performance | actual positive\n",
    "    model_recall = recall_score(df.actual, df[model], pos_label = positive)\n",
    "\n",
    "    accuracies[model] = model_accuracy\n",
    "    precisions[model] = model_precision\n",
    "    recalls[model] = model_recall\n",
    "        \n",
    "    print(model)\n",
    "    print(f'   model accuracy: {model_accuracy:.2%}')\n",
    "    print(f'   model recall: {model_recall:.2%}')\n",
    "    print(f'   model precision: {model_precision:.2%}')\n",
    "\n",
    "results_sci = pd.DataFrame(data = [accuracies, precisions, recalls], index = ['accuracy','precision','recall'])\n",
    "results_sci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "01c704aa-5615-4340-8387-151fba7c424e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.807400</td>\n",
       "      <td>0.630400</td>\n",
       "      <td>0.509600</td>\n",
       "      <td>0.742600</td>\n",
       "      <td>0.6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.689772</td>\n",
       "      <td>0.484122</td>\n",
       "      <td>0.358347</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.815006</td>\n",
       "      <td>0.890607</td>\n",
       "      <td>0.511455</td>\n",
       "      <td>0.345361</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model1    model2    model3    model4  baseline\n",
       "accuracy   0.807400  0.630400  0.509600  0.742600    0.6508\n",
       "precision  0.689772  0.484122  0.358347  0.807229       NaN\n",
       "recall     0.815006  0.890607  0.511455  0.345361    0.0000"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c5639b1a-3365-47d4-b699-373052969062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model1', 'model2', 'model3', 'model4', 'baseline']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f478b006-5bcc-4adc-aeff-6136a752b613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.69      0.82      0.75      1746\n",
      "\n",
      "   micro avg       0.69      0.82      0.75      1746\n",
      "   macro avg       0.69      0.82      0.75      1746\n",
      "weighted avg       0.69      0.82      0.75      1746\n",
      "\n",
      "-----\n",
      "model2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.48      0.89      0.63      1746\n",
      "\n",
      "   micro avg       0.48      0.89      0.63      1746\n",
      "   macro avg       0.48      0.89      0.63      1746\n",
      "weighted avg       0.48      0.89      0.63      1746\n",
      "\n",
      "-----\n",
      "model3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.36      0.51      0.42      1746\n",
      "\n",
      "   micro avg       0.36      0.51      0.42      1746\n",
      "   macro avg       0.36      0.51      0.42      1746\n",
      "weighted avg       0.36      0.51      0.42      1746\n",
      "\n",
      "-----\n",
      "model4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.81      0.35      0.48      1746\n",
      "\n",
      "   micro avg       0.81      0.35      0.48      1746\n",
      "   macro avg       0.81      0.35      0.48      1746\n",
      "weighted avg       0.81      0.35      0.48      1746\n",
      "\n",
      "-----\n",
      "baseline\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.00      0.00      0.00      1746\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      1746\n",
      "   macro avg       0.00      0.00      0.00      1746\n",
      "weighted avg       0.00      0.00      0.00      1746\n",
      "\n",
      "-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model)\n",
    "    print(classification_report(df.actual, df[model],labels = [positive]))\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "74239508-f07d-41f2-8495-8ccd906aef64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155b3ac9-0b95-4b57-9fbc-7ad5d4a3d284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python397jvsc74a57bd038cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
