{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de34af5-a3f2-4d3c-ac21-f79cf6b77aae",
   "metadata": {},
   "source": [
    "### Exercises from https://ds.codeup.com/classification/evaluation/#exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d340abd7-4b5d-4c5c-bf0e-cef6fd0b0370",
   "metadata": {},
   "source": [
    "## EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "319088e8-2bbe-47d3-8856-6d7d08d93fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec575792-df31-4434-a944-4691cde9529e",
   "metadata": {},
   "source": [
    "### 2. Confusion matrix:\n",
    "|               | pred dog   | pred cat   |\n",
    "|:------------  |-----------:|-----------:|\n",
    "| actual dog    |         46 |         7  |\n",
    "| actual cat    |         13 |         34 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc5b73-11a5-4b0e-b253-185240c156e7",
   "metadata": {},
   "source": [
    "False Positive: Predict cat but actually a dog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a1c271-0549-4c17-85f3-1932d2e681f2",
   "metadata": {},
   "source": [
    "False Negative: Predict dog but actually a cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ced4a1-bb17-43b9-99dd-f892f0dcea04",
   "metadata": {},
   "source": [
    "True Positive: Predicted cat and is a cat\n",
    "\n",
    "True Negative: Predicted dog and is a dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "000f627e-c2d9-4a21-b426-9427852ae12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = 7\n",
    "FN = 13\n",
    "TP = 34\n",
    "TN = 46"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7998db4f-1f35-4f8a-a755-9a6ec6c6e060",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "- **accuracy**: (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "- **precision**: TP / (TP + FP)\n",
    "\n",
    "- **recall**: TP / (TP + FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c546b81c-2770-4fb9-ae85-1dac2a4ce7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8, Precision: 0.8292682926829268, Recall: 0.723404255319149\n"
     ]
    }
   ],
   "source": [
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5239b794-0dc7-4f13-89e2-d9e5da569099",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Rubber ducks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a32830a8-a21f-404e-9a3e-9139a486ecfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        actual     model1     model2     model3\n",
       "0    No Defect  No Defect     Defect  No Defect\n",
       "1    No Defect  No Defect     Defect     Defect\n",
       "2    No Defect  No Defect     Defect  No Defect\n",
       "3    No Defect     Defect     Defect     Defect\n",
       "4    No Defect  No Defect     Defect  No Defect\n",
       "..         ...        ...        ...        ...\n",
       "195  No Defect  No Defect     Defect     Defect\n",
       "196     Defect     Defect  No Defect  No Defect\n",
       "197  No Defect  No Defect  No Defect  No Defect\n",
       "198  No Defect  No Defect     Defect     Defect\n",
       "199  No Defect  No Defect  No Defect     Defect\n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "df = pd.read_csv('c3.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e349bf-0645-4e8d-b3ca-6184681a4956",
   "metadata": {},
   "source": [
    "### Want to identify as many ducks with defect as possible. Which metric to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d9fb88-b603-4e9e-849e-67c7f7ba7bbc",
   "metadata": {},
   "source": [
    "- Positive: defect\n",
    "- Negative: no defect\n",
    "\n",
    "- False positive: predict defect but reality is no defect - customer still happy\n",
    "- False negative: predict no defect but reality is defect - customer unhappy\n",
    "\n",
    "Want to err on the side of not letting a single defective duck through. Want to catch as many of the positive cases as possible. False negative is more expensive as then we wouldn't catch a defect.\n",
    "\n",
    "###  -> USE RECALL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660bb2b5-2d6b-400e-800a-ed349838faa3",
   "metadata": {},
   "source": [
    "### Which model to go with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "37e1be98-f316-4772-8876-300ab62ffac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1\n",
      "   model accuracy: 95.00%\n",
      "\n",
      "   model recall: 50.00%\n",
      "\n",
      "model precision: 80.00%\n",
      "--------\n",
      "model2\n",
      "   model accuracy: 56.00%\n",
      "\n",
      "   model recall: 56.25%\n",
      "\n",
      "model precision: 10.00%\n",
      "--------\n",
      "model3\n",
      "   model accuracy: 55.50%\n",
      "\n",
      "   model recall: 81.25%\n",
      "\n",
      "model precision: 13.13%\n",
      "--------\n",
      "The model with the highest recall is model3\n"
     ]
    }
   ],
   "source": [
    "positive = 'Defect'\n",
    "models = ['model1','model2','model3']\n",
    "recalls={}\n",
    "precisions={}\n",
    "for model in models:\n",
    "    \n",
    "    # accuracy -- overall hit rate\n",
    "    model_accuracy = (df[model]== df.actual).mean()\n",
    "    # baseline_accuracy = (df.baseline == df.actual).mean()\n",
    "\n",
    "    # precision -- how good are our positive predictions?\n",
    "    # precision -- model performance | predicted positive\n",
    "    subset = df[df[model] == positive]\n",
    "    model_precision = (subset[model] == subset.actual).mean()\n",
    "    precisions[model] = model_precision\n",
    "    # subset = df[df.baseline == positive]\n",
    "    # baseline_precision = (subset.baseline == subset.actual).mean()\n",
    "\n",
    "    # recall -- how good are we at detecting actual positives?\n",
    "    # recall -- model performance | actual positive\n",
    "    subset = df[df.actual == positive]\n",
    "    model_recall = (subset[model] == subset.actual).mean()\n",
    "    recalls[model]=model_recall\n",
    "    # baseline_recall = (subset.baseline == subset.actual).mean()\n",
    "\n",
    "    print(f'{model}')\n",
    "    print(f'   model accuracy: {model_accuracy:.2%}')\n",
    "    # print(f'baseline accuracy: {baseline_accuracy:.2%}')\n",
    "    print()\n",
    "    print(f'   model recall: {model_recall:.2%}')\n",
    "    # print(f'baseline recall: {baseline_recall:.2%}')\n",
    "    print()\n",
    "    print(f'model precision: {model_precision:.2%}')\n",
    "    # print(f'baseline precision: {baseline_precision:.2%}')\n",
    "    print('--------')\n",
    "\n",
    "print(f'The model with the highest recall is {max(recalls, key = recalls.get)}')\n",
    "# print(f'The model with the highest precision is {max(precisions, key = precisions.get)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0a147f-7e7a-419b-ae04-881d907a2437",
   "metadata": {},
   "source": [
    "## -> USE MODEL 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4f623-54d8-4d95-baaf-70ff118eec44",
   "metadata": {},
   "source": [
    "### Going to give a vacation to those with defective duck\n",
    "### Really don't want to accidentally give someone a vacation package if it's not defective\n",
    "#### Reminder that false positive is predict defect but actually not defective, false negative is predict no defect but actually is defective\n",
    "#### False positive is more expensive -> we give a vacation to someone who got a duck without a defect \n",
    "### ->>> optimize for precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "eaa644a4-4aa9-4d8b-9573-e62d4a7092ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model with the highest precision is model1\n"
     ]
    }
   ],
   "source": [
    "print(f'The model with the highest precision is {max(precisions, key = precisions.get)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788259ef-6e6c-4171-9e36-82c0d1ad93c3",
   "metadata": {},
   "source": [
    "## -> USE MODEL 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315a0192-b0ad-4213-b112-6268e1d383d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Given the Gives you paws dataset use pandas to create a baseline model (i.e. a model that just predicts the most common class) and answer the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a49d243c-7182-4be5-ab83-8d512518fd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('gives_you_paws.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "bfabd45b-eb53-4356-a6ef-5049289fa72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4\n",
       "0    cat    cat    dog    cat    dog\n",
       "1    dog    dog    cat    cat    dog\n",
       "2    dog    cat    cat    cat    dog\n",
       "3    dog    dog    dog    cat    dog\n",
       "4    cat    cat    cat    dog    dog"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "02bc2b55-1cb6-4fcf-8438-dd9d27c4ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = df.actual.value_counts().idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d9cc43-27e9-43ab-87ff-2b655108f9d2",
   "metadata": {},
   "source": [
    "- positive = cat\n",
    "- negative = dog\n",
    "\n",
    "- false positive = predicts cat but dog\n",
    "- false negative = predicts dog but cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a5ad6ca6-b3d2-4e60-8582-523396020f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['baseline'] = baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "cbd47709-a9c1-4379-9b39-5e949505db32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6508"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.baseline == df.actual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "61dcc344-2520-4a23-a378-ca9d92db2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2bc6620a-b999-4329-8346-c0a722ce19ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.remove('actual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ebf7f67d-3760-4cfc-b93e-0c65f027670a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model1', 'model2', 'model3', 'model4', 'baseline']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "98140f34-41a1-471f-ad38-23a34dff3237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1\n",
      "   model accuracy: 80.74%\n",
      "   model recall: 80.33%\n",
      "   model precision: 89.00%\n",
      "model2\n",
      "   model accuracy: 63.04%\n",
      "   model recall: 49.08%\n",
      "   model precision: 89.32%\n",
      "model3\n",
      "   model accuracy: 50.96%\n",
      "   model recall: 50.86%\n",
      "   model precision: 65.99%\n",
      "model4\n",
      "   model accuracy: 74.26%\n",
      "   model recall: 95.57%\n",
      "   model precision: 73.12%\n",
      "baseline\n",
      "   model accuracy: 65.08%\n",
      "   model recall: 100.00%\n",
      "   model precision: 65.08%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.807400</td>\n",
       "      <td>0.630400</td>\n",
       "      <td>0.509600</td>\n",
       "      <td>0.742600</td>\n",
       "      <td>0.6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.890024</td>\n",
       "      <td>0.893177</td>\n",
       "      <td>0.659888</td>\n",
       "      <td>0.731249</td>\n",
       "      <td>0.6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.803319</td>\n",
       "      <td>0.490781</td>\n",
       "      <td>0.508605</td>\n",
       "      <td>0.955747</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model1    model2    model3    model4  baseline\n",
       "accuracy   0.807400  0.630400  0.509600  0.742600    0.6508\n",
       "precision  0.890024  0.893177  0.659888  0.731249    0.6508\n",
       "recall     0.803319  0.490781  0.508605  0.955747    1.0000"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive = 'dog'\n",
    "\n",
    "models = df.columns.to_list()\n",
    "models.remove('actual')\n",
    "accuracies= {}\n",
    "precisions = {}\n",
    "recalls = {}\n",
    "\n",
    "for model in models:\n",
    "    # accuracy -- overall hit rate\n",
    "    model_accuracy = (df[model] == df.actual).mean()\n",
    "\n",
    "    # precision -- how good are our positive predictions?\n",
    "    # precision -- model performance | predicted positive\n",
    "    subset = df[df[model] == positive]\n",
    "    model_precision = (subset[model] == subset.actual).mean()\n",
    "\n",
    "    # recall -- how good are we at detecting actual positives?\n",
    "    # recall -- model performance | actual positive\n",
    "    subset = df[df.actual == positive]\n",
    "    model_recall = (subset[model] == subset.actual).mean()\n",
    "\n",
    "    accuracies[model] = model_accuracy\n",
    "    precisions[model] = model_precision\n",
    "    recalls[model] = model_recall\n",
    "        \n",
    "    print(model)\n",
    "    print(f'   model accuracy: {model_accuracy:.2%}')\n",
    "    print(f'   model recall: {model_recall:.2%}')\n",
    "    print(f'   model precision: {model_precision:.2%}')\n",
    "\n",
    "results = pd.DataFrame(data = [accuracies, precisions, recalls], index = ['accuracy','precision','recall'])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c872ac94-7365-4708-822b-47525111f24c",
   "metadata": {},
   "source": [
    "In terms of accuracy, how do the various models compare to the baseline model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6298dd8c-0308-4cd0-95ea-fbcc61137427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models better than baseline based on accuracy: ['model1', 'model4']\n",
      "Models worse than baseline based on accuracy: ['model2', 'model3']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Models better than baseline based on accuracy: {results.columns[results.loc['accuracy'].baseline<results.loc['accuracy']].to_list()}\")\n",
    "print(f\"Models worse than baseline based on accuracy: {results.columns[results.loc['accuracy'].baseline>results.loc['accuracy']].to_list()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b21c9c-08a0-4a0b-b62a-e8d74143c791",
   "metadata": {},
   "source": [
    "Are any of the models better than the baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c65685a5-0f35-4713-9f1b-e0046553a569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models better than baseline based on accuracy: ['model1', 'model4']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Models better than baseline based on accuracy: {results.columns[results.loc['accuracy'].baseline<results.loc['accuracy']].to_list()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3316ebd2-fe90-4c73-9de3-1c381bcf2cea",
   "metadata": {},
   "source": [
    "- positive = cat\n",
    "- negative = dog\n",
    "\n",
    "- false positive = predicts cat but dog\n",
    "- false negative = predicts dog but cat\n",
    "\n",
    "- Phase 1: determine dog or cat\n",
    "- Phase 2: Another review, presented to users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a262c58c-3e77-4175-bf1e-fd98309f29b7",
   "metadata": {},
   "source": [
    "### Suppose you are working on a team that solely deals with dog pictures. Which of these models would you recomend for Phase I? For Phase II?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fd5eb0-ba7c-4a72-a2d7-381df7bb8a6b",
   "metadata": {},
   "source": [
    "This means positive = dog, negative is cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06870dbe-82e6-47dd-934b-fbf0486ef2d0",
   "metadata": {},
   "source": [
    "### From review:\n",
    "\n",
    "Phase 1: don't miss out on cat pictures\n",
    "Phase 2: make sure don't show any cats\n",
    "Recall best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4183ec-72c7-477b-844f-85d72fab3073",
   "metadata": {},
   "source": [
    "#### I want Phase I model to let more actual cats through but predicted dogs so we can correct - happier with a false positive. False negative more expensive as then I don't have chance to correct. Optimize for recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fc85ae6d-be5d-4d19-a0d1-13d7df42c250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model based on recall: model2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best model based on recall: {max(recalls, key = recalls.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b70a0a-87bd-47af-8ac1-d3504711d524",
   "metadata": {},
   "source": [
    "### From review for phase 2: optimize for precision, model 2 REVIEW CALCULATIONS???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f225565-e7a4-4489-be1d-4369c4b22003",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### I want Phase II model to avoid sending any cats through - happier with a false positive. False negative more expensive as then the customer sees a cat!! Optimize for recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "516853f9-8fba-46f1-88ce-0c77e045985c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model based on precision: model4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best model based on precision: {max(precisions, key = precisions.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d656646-e361-44e8-8399-7fe8cafb06e6",
   "metadata": {},
   "source": [
    "### Suppose you are working on a team that solely deals with cat pictures. Which of these models would you recomend for Phase I? For Phase II?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add2fe1d-3a7b-4ddc-bd21-2561f83bd7f0",
   "metadata": {},
   "source": [
    "#### I want Phase I model to let more actual dogs through but predicted cats so we can correct - happier with a false positive. False negative more expensive as then I don't have chance to correct. Optimize for recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fbdfcbca-bff6-4d09-a0cd-49e076fdc038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model based on recall: model2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best model based on recall: {max(recalls, key = recalls.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a702c78d-7aa3-4406-ada3-2b643f4a6487",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### I want Phase II model to avoid sending any dogs through - happier with a false negative. False positive more expensive as then the customer sees a dog!! Optimize for precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1fcec92d-608b-413b-bda3-dfbd5c83790b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model based on precision: model4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best model based on precision: {max(precisions, key = precisions.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d641cd1c-556c-47c4-8d1c-8d06303fae36",
   "metadata": {},
   "source": [
    "### Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6dd25c7f-cf12-4367-a425-77c1781029d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1\n",
      "   model accuracy: 80.74%\n",
      "   model recall: 81.50%\n",
      "   model precision: 68.98%\n",
      "model2\n",
      "   model accuracy: 63.04%\n",
      "   model recall: 89.06%\n",
      "   model precision: 48.41%\n",
      "model3\n",
      "   model accuracy: 50.96%\n",
      "   model recall: 51.15%\n",
      "   model precision: 35.83%\n",
      "model4\n",
      "   model accuracy: 74.26%\n",
      "   model recall: 34.54%\n",
      "   model precision: 80.72%\n",
      "baseline\n",
      "   model accuracy: 65.08%\n",
      "   model recall: 0.00%\n",
      "   model precision: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.807400</td>\n",
       "      <td>0.630400</td>\n",
       "      <td>0.509600</td>\n",
       "      <td>0.742600</td>\n",
       "      <td>0.6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.689772</td>\n",
       "      <td>0.484122</td>\n",
       "      <td>0.358347</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.815006</td>\n",
       "      <td>0.890607</td>\n",
       "      <td>0.511455</td>\n",
       "      <td>0.345361</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model1    model2    model3    model4  baseline\n",
       "accuracy   0.807400  0.630400  0.509600  0.742600    0.6508\n",
       "precision  0.689772  0.484122  0.358347  0.807229    0.0000\n",
       "recall     0.815006  0.890607  0.511455  0.345361    0.0000"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive = 'cat'\n",
    "\n",
    "models = df.columns.to_list()\n",
    "models.remove('actual')\n",
    "accuracies= {}\n",
    "precisions = {}\n",
    "recalls = {}\n",
    "\n",
    "for model in models:\n",
    "    # accuracy -- overall hit rate\n",
    "    model_accuracy = accuracy_score(df.actual, df[model])\n",
    "\n",
    "    # precision -- how good are our positive predictions?\n",
    "    # precision -- model performance | predicted positive\n",
    "    model_precision = precision_score(df.actual, df[model], pos_label=positive)\n",
    "\n",
    "    # recall -- how good are we at detecting actual positives?\n",
    "    # recall -- model performance | actual positive\n",
    "    model_recall = recall_score(df.actual, df[model], pos_label = positive)\n",
    "\n",
    "    accuracies[model] = model_accuracy\n",
    "    precisions[model] = model_precision\n",
    "    recalls[model] = model_recall\n",
    "        \n",
    "    print(model)\n",
    "    print(f'   model accuracy: {model_accuracy:.2%}')\n",
    "    print(f'   model recall: {model_recall:.2%}')\n",
    "    print(f'   model precision: {model_precision:.2%}')\n",
    "\n",
    "results_sci = pd.DataFrame(data = [accuracies, precisions, recalls], index = ['accuracy','precision','recall'])\n",
    "results_sci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "01c704aa-5615-4340-8387-151fba7c424e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.807400</td>\n",
       "      <td>0.630400</td>\n",
       "      <td>0.509600</td>\n",
       "      <td>0.742600</td>\n",
       "      <td>0.6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.689772</td>\n",
       "      <td>0.484122</td>\n",
       "      <td>0.358347</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.815006</td>\n",
       "      <td>0.890607</td>\n",
       "      <td>0.511455</td>\n",
       "      <td>0.345361</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model1    model2    model3    model4  baseline\n",
       "accuracy   0.807400  0.630400  0.509600  0.742600    0.6508\n",
       "precision  0.689772  0.484122  0.358347  0.807229       NaN\n",
       "recall     0.815006  0.890607  0.511455  0.345361    0.0000"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c5639b1a-3365-47d4-b699-373052969062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model1', 'model2', 'model3', 'model4', 'baseline']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f478b006-5bcc-4adc-aeff-6136a752b613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1\n",
      "              precision    recall  f1-score    support\n",
      "cat            0.689772  0.815006  0.747178  1746.0000\n",
      "dog            0.890024  0.803319  0.844452  3254.0000\n",
      "accuracy       0.807400  0.807400  0.807400     0.8074\n",
      "macro avg      0.789898  0.809162  0.795815  5000.0000\n",
      "weighted avg   0.820096  0.807400  0.810484  5000.0000\n",
      "-----\n",
      "model2\n",
      "              precision    recall  f1-score    support\n",
      "cat            0.484122  0.890607  0.627269  1746.0000\n",
      "dog            0.893177  0.490781  0.633479  3254.0000\n",
      "accuracy       0.630400  0.630400  0.630400     0.6304\n",
      "macro avg      0.688649  0.690694  0.630374  5000.0000\n",
      "weighted avg   0.750335  0.630400  0.631310  5000.0000\n",
      "-----\n",
      "model3\n",
      "              precision    recall  f1-score    support\n",
      "cat            0.358347  0.511455  0.421425  1746.0000\n",
      "dog            0.659888  0.508605  0.574453  3254.0000\n",
      "accuracy       0.509600  0.509600  0.509600     0.5096\n",
      "macro avg      0.509118  0.510030  0.497939  5000.0000\n",
      "weighted avg   0.554590  0.509600  0.521016  5000.0000\n",
      "-----\n",
      "model4\n",
      "              precision    recall  f1-score    support\n",
      "cat            0.807229  0.345361  0.483755  1746.0000\n",
      "dog            0.731249  0.955747  0.828560  3254.0000\n",
      "accuracy       0.742600  0.742600  0.742600     0.7426\n",
      "macro avg      0.769239  0.650554  0.656157  5000.0000\n",
      "weighted avg   0.757781  0.742600  0.708154  5000.0000\n",
      "-----\n",
      "baseline\n",
      "              precision  recall  f1-score    support\n",
      "cat            0.000000  0.0000  0.000000  1746.0000\n",
      "dog            0.650800  1.0000  0.788466  3254.0000\n",
      "accuracy       0.650800  0.6508  0.650800     0.6508\n",
      "macro avg      0.325400  0.5000  0.394233  5000.0000\n",
      "weighted avg   0.423541  0.6508  0.513134  5000.0000\n",
      "-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model)\n",
    "    print(pd.DataFrame(classification_report(df.actual, df[model], output_dict =True)).T)#,labels = [positive]))\n",
    "    \n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcc39d3-408e-45bb-a974-c79ed22b3121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python397jvsc74a57bd038cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
